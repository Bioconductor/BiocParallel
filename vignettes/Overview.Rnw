%\VignetteIndexEntry{Introduction to BiocParallel}
%\VignetteKeywords{parallel, Infrastructure}
%\VignettePackage{BiocParallel}
%\VignetteEngine{knitr::knitr}


\documentclass{article}

<<style, eval=TRUE, echo=FALSE, results="asis">>=
BiocStyle::latex()
@ 

\newcommand{\BiocParallel}{\Biocpkg{BiocParallel}}

\title{Introduction to \BiocParallel} 
\author{Vincent Carey, Michael Lawrence, Martin
  Morgan\footnote{\url{mtmorgan@fhcrc.org}}} 
\date{Edited: June 13, 2014; Compiled: \today}

\begin{document}

\maketitle

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Numerous approaches are available for parallel computing in \R. The Task 
View document at
\url{http://cran.r-project.org/web/views/HighPerformanceComputing.html}
lists various package most of which cite or identify one or more of 
\CRANpkg{snow}, \CRANpkg{Rmpi} or \CRANpkg{foreach} as relevant
parallelization infrastructure. 

The \BiocParallel{} package provides a unified approach to specifying
parallel evaluation while allowing for multiple back-ends. A basic
objective of \BiocParallel{} is to reduce the complexity faced
when developing and using software that performs parallel computations.
\BiocParallel{} introduces ``parameter'' arguments used at runtime
to define the method of parallel execution. This allows developers to 
focus on \textit{what} is to be computed, leaving the \textit{how} 
to the infrastructure.

Tools available in \BiocParallel{} offer the following advantages over 
\textit{ad hoc} \R{} programming for parallel computation.

\begin{description}
  \item{uniform idiom: } 
    \Rcode{BiocParallelParam} instances define parallel computing 
    resources such as number of workers, error handling, cleanup, etc. 
    Sensible default definitions are generated when the package is loaded.

  \item{iterative and vectorized operations: }
    \Rcode{bplapply} and \Rcode{bpvec} address iteration and vectorized
    operations in parallel respectively.

  \item{cluster scheduling: }
    When the parallel environment is managed by a cluster scheduler
    through \CRANpkg{BatchJobs}, job management and result retrieval are
    considerably simplified.

  \item{support of \Rcode{foreach}: }
    The \CRANpkg{foreach} and \CRANpkg{iterators} packages are fully supported 
    but registration of the parallel back end uses \Rcode{BiocParallelParam} 
    instances.
\end{description}

In this vignette we illustrate the use of these tools in various computing
environments.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Quick start}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The \Rpackage{BiocParallel} package is available at bioconductor.org
and can be downloaded via \Rcode{biocLite}:
<<biocLite, eval=FALSE>>=
source("http://bioconductor.org/biocLite.R")
biocLite("BiocParallel")
@

Load \BiocParallel{}.
<<BiocParallel>>=
library(BiocParallel)
@ 

The test function simply returns the square root of ``x''.
<<quickstart_FUN>>=
FUN <- function(x) { round(sqrt(x), 4) }
@
 
Functions in \BiocParallel use the registered back-ends for parallel
evaluation. The default is the top entry of the registry list.
<<quickstart_registry>>=
registered()
@

When a \BiocParallel{} function is invoked with no \Rcode{BPPARAM} 
argument the default back-end is used.
<<quickstart_bplapply_default, eval=FALSE>>=
bplapply(1:4, FUN)
@

Environment specific back-ends can be specified for any of the
registry entries. Here we create a 2-worker PSOCK cluster and
use it inplace of the default.
<<quickstart_snow>>=
param <- SnowParam(workers = 2, type = "PSOCK") 
bplapply(1:4, FUN, BPPARAM = param)
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The \BiocParallel{} Interface}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Classes}

\subsubsection{\Rcode{BiocParallelParam}}

\Rcode{BiocParallelParam} instances describe parallel evaluation environments. 
Different types of parallel computation are supported by creating and 
\Rcode{register()}ing a `\Rcode{Param}'. Supported \Rcode{Param} objects are:

\begin{itemize}
  \item{\Rcode{SerialParam}:} 
    Evaluate \BiocParallel-enabled code with parallel evaluation disabled. 
    This is very useful when writing new scripts and trying to debug code.

  \item{\Rcode{MulticoreParam}:} 
    Evaluate \BiocParallel-enabled code using multiple cores on a single 
    computer. When available, this is the most efficient and least 
    troublesome way to parallelize code. Unfortunately, Windows does not 
    support multi-core evaluation (the \Rcode{MulticoreParam} object can 
    be used, but evaluation is serial). On other operating systems, the 
    default number of workers equals the value of the global option 
    \Rcode{mc.cores} (e.g., \Rcode{getOption("mc.cores")}) or, if that is 
    not set, the number of cores returned by \Rcode{parallel::detectCores()}.

  \item{\Rcode{SnowParam}:}
    Evaluate \BiocParallel-enabled code across several distinct \R{} 
    instances, on one or several computers. This can be an easy way to 
    parallelize code when working with one or several computers, and is 
    based on facilities originally implemented in the \CRANpkg{snow} package. 
    Different types of \CRANpkg{snow} `back-ends' are supported, including 
    socket and MPI clusters.

  \item{\Rcode{BatchJobsParam}:}
    Evaluate \BiocParallel-enabled code by submitting to a cluster scheduler 
    like SGE. 

  \item{\Rcode{DoparParam}:}
    Register a parallel back-end supported by the \CRANpkg{foreach} package 
    for use with \BiocParallel.
\end{itemize}

The simplest illustration of creating \Rcode{BiocParallelParam} is
<<BiocParallelParam_SerialParam>>=
serialParam <- SerialParam()
serialParam
@ 
 
Most parameters have additional arguments influencing behavior, e.g.,
specifying the number of `cores' to use when creating a
\Rcode{MulticoreParam} instance
<<BiocParallelParam_MulticoreParam>>=
multicoreParam <- MulticoreParam(workers = 8)
multicoreParam
@ 
 
Arguments are detailed on the corresponding help page, e.g.,
\Rcode{?MulticoreParam}.

\subsubsection{\Rcode{register()}ing \Rcode{BiocParallelParam} instances}

The list of registered \Rcode{BiocParallelParam} instances represents
the user's preferences for different types of back-ends. Individual
algorithms may specify a preferred back-end, and different back-ends
maybe chosen when parallel evaluation is nested.

The registry behaves like a `stack' in that the last entry registered
is added to the top of the list and becomes the ``next used`` 
(i.e., the default).

\Rcode{registered} invoked with no arguments lists all back-ends.
<<register_registered>>=
registered()
@

\Rcode{bpparam} returns the default from the top of the list.
<<register_bpparam>>=
bpparam()
@

Add a specialized instance with \Rcode{register}. When 
\Rcode{default} is TRUE, the new instance becomes the default.
<<register_BatchJobsParam>>=
register(BatchJobsParam(workers = 10), default = TRUE)
@

BatchJobsParam has been moved to the top of the list and
is now the default.
<<register_BatchJobsParam2>>=
names(registered())
bpparam()
@ 

\subsection{Functions}

%% TODO: mention functions called from pkg dependencies
 
\subsubsection{Parallel looping, vectorized and aggregate operations}

These are used in common functions, implemented as much as possible
for all back-ends. The functions (see the help pages, e.g.,
\Rcode{?bplapply} for a full definition) include

\begin{description}
  \item{\Rcode{bplapply(X, FUN, ...)}: } 
    Apply in parallel a function \Rcode{FUN} to each element of \Rcode{X}. 
    \Rcode{bplapply} invokes \Rcode{FUN} \Rcode{length(X)} times, each 
    time with a single element of \Rcode{X}.

  \item{\Rcode{bpmapply(FUN, ...)}: }
    Apply in parallel a function \Rcode{FUN} to the first, second, etc., 
    elements of each argument in \ldots.

  \item{\Rcode{bpvec(X, FUN, ...)}: } 
    Apply in parallel a function \Rcode{FUN} to subsets of \Rcode{X}. 
    \Rcode{bpvec} invokes function \Rcode{FUN} as many times as there are 
    cores or cluster nodes, with \Rcode{FUN} receiving a subset (typically 
    more than 1 element, in contrast to \Rcode{bplapply}) of \Rcode{X}.

  \item{\Rcode{bpaggregate(x, data, FUN, ...)}: }
    Use the formula in \Rcode{x} to aggregate \Rcode{data} using \Rcode{FUN}.
\end{description}

\subsubsection{Parallel evaluation environment}

These functions query and control the state of the parallel evaluation 
environment.

\begin{description}
  \item{\Rcode{bpisup(x)}: } 
    Query a \Rcode{BiocParallelParam} back-end \Rcode{x} for its status.

  \item{\Rcode{bpworkers}: }
    Query a \Rcode{BiocParallelParam} back-end for the number of workers 
    available for parallel evaluation.

  \item{\Rcode{bpstart(x)}: } 
    Start a parallel back end specified by \Rcode{BiocParallelParam} \Rcode{x}, 
    if possible.

  \item{\Rcode{bpstop(x)}: } 
    Stop a parallel back end specified by \Rcode{BiocParallelParam} \Rcode{x}.
\end{description}

\subsubsection{Error recovery}
The following assist in recovering from errors.

\begin{description}
  \item{\Rcode{bplasterror}: }
    Report the last error reported from a \BiocParallel{} evaluation.

  \item{\Rcode{bpresume}: }
    Attempt to resume computation after an error.
\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Use cases}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% TODO: advanced use of BPPARAM as list for nested calls to bplapply

Sample data are bam files from a transcription profiling experiment
available in the \Rpackage{RNAseqData.HNRNPC.bam.chr14} package.

<<use_cases_data>>=
library(RNAseqData.HNRNPC.bam.chr14)
fls <- RNAseqData.HNRNPC.bam.chr14_BAMFILES
@

\subsection{Single machine}

In this section we count overlaps between regions of interest and
BAM files in both shared and non-shared memory environments.

\subsubsection{Forking: MultiCoreParam}

There are substantial benefits, such as shared memory, to be had using 
multiple cores on a single machine. On a single non-Windows machine the 
recommended back-end is multi-core which uses forked processes. 

Shared memory eliminates the need to pass large data between workers 
and load common packages. The code we write in this section will take
advantage of these aspects.

First create a GRanges with regions of interest (in practice this 
could be large).
<<forking_gr, message=FALSE>>=
library(GenomicAlignments) ## for GenomicRanges and readGAlignments()
gr <- GRanges("chr14", IRanges((1000:3999)*5000, width=1000))
@

For counting, we want to extract the BAM file regions that span
the ranges in `gr`. The `param` object defines this region.
<<forking_param>>=
param <- ScanBamParam(which=range(gr))
@

The function executed on each worker counts overlaps between the 
ranges in `gr` and the regions of the file defined by `param`.
<<forking_FUN>>=
FUN <- function(fl, param) {
  gal <- readGAlignments(fl, param = param)
  sum(countOverlaps(gr, gal))
}
@

Call \Rcode{bplapply} with a \Rcode{MulticoreParam}.
\begin{verbatim}
> bplapply(fls[1:3], FUN, BPPARAM = MulticoreParam(), param = param)
$ERR127306
[1] 1185

$ERR127307
[1] 1123

$ERR127308
[1] 1241
\end{verbatim}

Note that the GRanges object was not explictly passed in the call
to \Rcode{bplapply} and FUN did not have to load the 
\Biocpkg{GenomicAlignments} package for access to the 
\Rcode{readGAlignments} function.

\subsubsection{Clusters: SnowParam, DoparParam}
On a Windows machine forking is not an option and processes must be 
spawned via a cluster. \BiocParallel{} back-end choices are a Snow 
cluster configured with \Rclass{SnowParam} or the \Rclass{DoparParam} 
for use with the \Rpackage{foreach} package.

To run the same example on a cluster we need to pass the GRanges data 
to FUN and load the \Biocpkg{GenomicAlignments} package.
<<cluster_FUN>>=
FUN <- function(fl, param, gr) {
  library(GenomicAlignments)
  gal <- readGAlignments(fl, param = param)
  sum(countOverlaps(gr, gal))
}
@

Specify a Snow cluster back-end with 2 workers. The default `type` is PSOCK.
<<cluster_snow_param>>=
snow <- SnowParam(workers = 2)
@

Call \Rcode{bplapply} with the \Rcode{SnowParam} and pass the GRangesList
as the argument to split. 
<<cluster_bplapply>>=
bplapply(fls[1:3], FUN, BPPARAM = snow, param = param, gr = gr)
@

The FUN written for the cluster adds some overhead due to the passing of the 
GRanges and time due to the loading of \Biocpkg{GenomicAlignments} on each 
worker. This approach, however, has the advantage that it works on 
most platforms and does not require a coding change when switching between 
windows and non-windows machines. 

\subsection{\emph{Ad hoc} clusters}

\subsection{Clusters with schedulers}

Computer clusters are far from standardized, so the following may
require significant adaptation; it is written from experience here at
FHCRC, where we have a large cluster managed via SLURM. Nodes on the
cluster have shared disks and common system images, minimizing
complexity about making data resources available to individual nodes.
There are two simple models for use of the cluster.

\paragraph{Cluster-centric} The idea is use cluster management
software to allocate resources, and then arrange for an \R{} script to
be evaluated in the context of allocated resources. For SLURM, we
might request space for 4 tasks (with \verb+salloc+ or \verb+sbatch+),
arrange to start the MPI environment (with \verb+orterun+) and on a
single node in that universe run an \R{} script
\verb+BiocParallel-MPI.R+. The command is
\begin{verbatim}
$ salloc -N 4 orterun -n 1 R -f BiocParallel-MPI.R
\end{verbatim}
The \R{} script might do the following, using MPI for parallel
evaluation. We start by loading necessary packages and defining
\Rcode{FUN} work to be done
<<cluster-MPI-work>>=
library(BiocParallel)
library(Rmpi)
FUN <- function(i) system("hostname", intern=TRUE)
@ 
%% 
Create a \Rclass{SnowParam} instance with the number of nodes equal to
the size of the MPI universe minus 1 (let one node dispatch jobs to
workers), and register this instance as the default
<<cluster-MPI, eval=FALSE>>=
param <- SnowParam(mpi.universe.size() - 1, "MPI")
register(param)
@ 
%% 
Evaluate the work in parallel, process the results, clean up, and quit
<<cluster-MPI-do, eval=FALSE>>=
xx <- bplapply(1:100, FUN)
table(unlist(x))
mpi.quit()
@ 
%% 
The entire session is as follows:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
$ salloc -N 4 orterun -n 1 R --vanilla -f BiocParallel-MPI.R 
salloc: Job is in held state, pending scheduler release
salloc: Pending job allocation 6762292
salloc: job 6762292 queued and waiting for resources
salloc: job 6762292 has been allocated resources
salloc: Granted job allocation 6762292
## ...
> FUN <- function(i) system("hostname", intern=TRUE)
> 
> library(BiocParallel)
> library(Rmpi)
> param <- SnowParam(mpi.universe.size() - 1, "MPI")
> register(param)
> xx <- bplapply(1:100, FUN)
	4 slaves are spawned successfully. 0 failed.
> table(unlist(xx))

gizmof13 gizmof71 gizmof86 gizmof88 
      25       25       25       25 
> 
> mpi.quit()
salloc: Relinquishing job allocation 6762292
salloc: Job allocation 6762292 has been revoked.
\end{verbatim}
\end{kframe}
\end{knitrout}
One advantage of this approach is that the responsibility for managing
the cluster lies firmly with the cluster management software -- if one
wants more nodes, or needs special resources, then adjust parameters
to \verb+salloc+ (or \verb+sbatch+).

Notice that workers are spawned within the \Rcode{bplapply} function;
it might often make sense to more explicitly manage workers with
\Rfunction{bpstart} and \Rfunction{bpstop}, e.g.,
<<cluster-MPI-bpstart, eval=FALSE>>=
param <- bpstart(SnowParam(mpi.universe.size() - 1, "MPI"))
register(param)
xx <- bplapply(1:100, FUN)
bpstop(param)
mpi.quit()
@ 

\paragraph{R-centric} A more \R-centric approach might start an \R{}
script on the head node, and use \Rpackage{BatchJobs} to submit jobs
from within the \R{} session. One way of doing this is to create a
file containing a template for the job submission step, e.g., for
SLURM\footnote{see
  \url{https://github.com/tudo-r/BatchJobs/tree/master/examples/cfSLURM}}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
#!/bin/bash
##
## file: slurm.tmpl
## Job Resource Interface Definition
##
## ntasks [integer(1)]: Number of required tasks.
## ncpus [integer(1)]: Number of required cpus per task.
## walltime [integer(1)]: Walltime for this job, in minutes.
##
## 'resources' is an argument provided to BatchJobsParam()

#SBATCH --job-name=<%= job.name %>
#SBATCH --output=<%= log.file %>
#SBATCH --error=<%= log.file %>
#SBATCH --ntasks=<%= resources$ntasks %>
#SBATCH --cpus-per-task=<%= resources$ncpus %>
#SBATCH --time=0:10:0

## Run R: we merge R output with stdout from SLURM, which gets then
## logged via --output option
R CMD BATCH --no-save --no-restore "<%= rscript %>" /dev/stdout
\end{verbatim}
\end{kframe}
\end{knitrout}
The \R{} script, run interactively or from the command line, might
then look like
<<cluster-BatchJobs, eval=FALSE>>=
## define work to be done
FUN <- function(i) system("hostname", intern=TRUE)

library(BiocParallel)
library(BatchJobs)

## register SLURM cluster instructions from the template file
funs <- makeClusterFunctionsSLURM("slurm.tmpl")
param <- BatchJobsParam(4, resources=list(ncpus=1),
                        cluster.functions=funs)
register(param)

## do work
xx <- bplapply(1:100, FUN)
table(unlist(xx))
@ 
%% 
The code runs on the head node until \Rcode{bplapply}, where the \R{}
script interacts with the SLURM scheduler to request a SLURM
allocation, run jobs, and retrieve results. The argument \Rcode{4} to
\Rcode{BatchJobsParam} specifies the number of workers to request from
the scheduler; \Rcode{bplapply} divides the 100 jobs among the 4
workers. If \Rcode{BatchJobsParam} had been created without specifying
any workers, then 100 jobs implied by the argument to \Rcode{bplapply}
would be associated with 100 tasks submitted to the scheduler.

Because cluster tasks are running in independent \R{} instances, and
often on physically separate machines, a convenient `best practice'
is to write \Rcode{FUN} in a `functional programming' manner, such
that all data required for the function is passed in as arguments or
(for large data) loaded implicitly or explicitly (e.g., via an \R{}
library) from disk.

\subsection{\Bioconductor{} Amazon Machine Image (AMI)}

A \Bioconductor{} AMI is available at
\url{http://www.bioconductor.org/help/bioconductor-cloud-ami/#using_cluster}.

TODO

\section{For developers}

Developers wishing to use \BiocParallel{} in their own packages should
include \BiocParallel{} in the \texttt{DESCRIPTION} file
\begin{verbatim}
    Imports: BiocParallel
\end{verbatim}
and import the functions they wish to use in the \texttt{NAMESPACE}
file, e.g.,
\begin{verbatim}
    importFrom(BiocParallel, bplapply)
\end{verbatim}
Then invoke the desired function in the code, e.g.,
<<devel-bplapply>>=
system.time(x <- bplapply(1:3, function(i) { Sys.sleep(i); i }))
unlist(x)
@ 
%%
This will use the back-end returned by \Rcode{bpparam()}, by default a
\Rcode{MulticoreParam()} instance or the user's preferred back-end if
they have used \Rcode{register()}.  The \Rcode{MulticoreParam} back-end
does not require any special configuration or set-up and is therefore
the safest option for developers. Unfortunately,
\Rcode{MulticoreParam} provides only serial evaluation on Windows.

Developers should document that their function uses \BiocParallel{}
functions on the man page, and should perhaps include in their
function signature an argument \Rcode{BPPARAM=bpparam()}.

Developers wishing to invoke back-ends other than
\Rcode{MulticoreParam} need to take special care to ensure that
required packages, data, and functions are available and loaded on the
remote nodes.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Considerations for big data in genomics}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Limiting resource consumption}

\subsection{Restricting queries}

\subsection{Iterating}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\Rcode{sessionInfo()}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

<<sessionInfo>>=
toLatex(sessionInfo())
@

\end{document}
